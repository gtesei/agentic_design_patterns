# Prompt Chaining - Quick Start Guide

## ğŸš€ Get Started in 3 Minutes

### Step 1: Navigate to the Directory
```bash
cd foundational_design_patterns/1_prompt_chain
```

### Step 2: Run the Example
```bash
bash run.sh
```

---

## ğŸ“– Understanding Prompt Chaining in 30 Seconds

**Prompt Chaining** breaks complex tasks into a sequence of simpler steps:

```
Input â†’ Step 1 â†’ Step 2 â†’ Step 3 â†’ Final Output
```

Each step focuses on ONE thing, making the process:
- More reliable (fewer errors)
- More transparent (see each step)
- Easier to debug (find problems faster)

---

## ğŸ¯ What This Example Does

The example demonstrates a **content generation pipeline**:

1. **Research** - Gather key facts about a topic
2. **Outline** - Create a structured outline
3. **Draft** - Write the initial content
4. **Edit** - Refine and improve the draft

---

## ğŸ’¡ Example Output Structure

```
Topic: "Climate Change"
    â†“
Research Step: "Key facts: greenhouse gases, temperature rise..."
    â†“
Outline Step: "I. Introduction II. Causes III. Effects..."
    â†“
Draft Step: "Climate change refers to long-term shifts..."
    â†“
Edit Step: "Climate change represents one of the most..."
```

---

## ğŸ”§ Key Concepts

### Sequential Processing
Each step waits for the previous step to complete before starting.

### Focused Prompts
Each step has a specific, narrow task instead of trying to do everything at once.

### Intermediate Outputs
You can inspect, validate, or modify outputs between steps.

### Error Isolation
If something fails, you know exactly which step had the problem.

---

## ğŸ¨ When to Use Prompt Chaining

âœ… **Good For:**
- Multi-step document processing
- Content generation workflows
- Data transformation pipelines
- Sequential reasoning tasks

âŒ **Not Ideal For:**
- Simple single-step tasks
- Real-time applications (adds latency)
- Tasks needing all context at once

---

## ğŸ› ï¸ Customization Tips

### Modify the Pipeline

Edit `src/chain_prompt.py` to change steps:

```python
# Add a new step
def new_step(input_text):
    prompt = f"Process this: {input_text}"
    return llm.invoke(prompt)

# Add to chain
result = step1(input) | step2 | new_step | step3
```

### Change the Topic

Modify the input in the main function:

```python
topic = "Your Custom Topic Here"
```

### Add Validation

Insert validation between steps:

```python
research = research_step(topic)
if len(research) < 100:
    research = research_step(topic)  # Retry
outline = outline_step(research)
```

---

## ğŸ“Š Performance Notes

- **Latency**: Each step adds ~1-3 seconds (sequential)
- **Tokens**: Each step uses separate tokens
- **Quality**: Higher quality than single-step approach
- **Cost**: 3-5x more expensive than single prompt

**Trade-off**: Better quality and reliability vs. higher cost and latency

---

## ğŸ” Debugging Tips

1. **Print intermediate outputs** to see what each step produces
2. **Test each step independently** before chaining
3. **Check token usage** for each step
4. **Validate outputs** between steps
5. **Use smaller models** for non-critical steps

---

## ğŸ“š Learn More

- **Full Documentation**: See [README.md](./README.md)
- **Main Repository**: See [../../README.md](../../README.md)
- **Related Patterns**:
  - Pattern 4 (Reflection) - Quality improvement
  - Pattern 6 (Planning) - Complex orchestration

---

## ğŸ“ Next Steps

1. âœ… Run the basic example
2. âœ… Modify the topic and see results
3. âœ… Add a custom step to the chain
4. âœ… Try different validation strategies
5. âœ… Combine with other patterns

---

**Pattern Type**: Sequential Processing

**Complexity**: â­â­ (Beginner-Friendly)

**Best For**: Multi-step transformations
