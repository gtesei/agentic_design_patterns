"""
Resource Optimization Pattern: Advanced Implementation

This example demonstrates advanced optimization techniques:
- Request batching for parallel processing
- Cost-aware model routing with quality fallback
- Adaptive prompt optimization
- Predictive caching based on patterns
- Real-time optimization decisions
- A/B testing simulation

Problem: Complex, high-volume systems need sophisticated optimization strategies
Solution: Adaptive batching, predictive caching, cost-aware routing with quality thresholds
"""


import sys

# Add parent directory to path to import ssl_fix
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../../.."))
import ssl_fix  # Apply SSL bypass for corporate networks


import asyncio
import hashlib
import os
import random
import time
from collections import defaultdict, deque
from datetime import datetime, timedelta
from typing import Optional

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# Load environment variables
load_dotenv(os.path.join(os.path.dirname(__file__), "../../../.env"))

# Initialize models
gpt4 = ChatOpenAI(model="gpt-4", temperature=0)
gpt4_mini = ChatOpenAI(model="gpt-4o-mini", temperature=0)
gpt35 = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)


# ===========================
# Predictive Cache
# ===========================

class PredictiveCache:
    """Cache with pattern learning and predictive prefetching"""

    def __init__(self, ttl_seconds: int = 3600):
        self.cache: dict[str, tuple[str, datetime]] = {}
        self.query_history: deque = deque(maxlen=100)
        self.pattern_counts: dict[str, dict[str, int]] = defaultdict(lambda: defaultdict(int))
        self.ttl_seconds = ttl_seconds
        self.hits = 0
        self.misses = 0
        self.prefetch_hits = 0

    def _generate_key(self, prompt: str, model: str) -> str:
        """Generate cache key"""
        return hashlib.md5(f"{prompt}|{model}".encode()).hexdigest()

    def get(self, prompt: str, model: str) -> Optional[str]:
        """Get cached response"""
        key = self._generate_key(prompt, model)

        if key in self.cache:
            response, timestamp = self.cache[key]
            if datetime.now() - timestamp < timedelta(seconds=self.ttl_seconds):
                self.hits += 1
                return response
            else:
                del self.cache[key]

        self.misses += 1
        return None

    def set(self, prompt: str, model: str, response: str):
        """Store response and learn patterns"""
        key = self._generate_key(prompt, model)
        self.cache[key] = (response, datetime.now())

        # Learn query patterns
        if self.query_history:
            prev_query = self.query_history[-1]
            self.pattern_counts[prev_query][prompt] += 1

        self.query_history.append(prompt)

    def predict_next_queries(self, current_query: str, top_n: int = 3) -> list[str]:
        """Predict likely next queries based on patterns"""
        if current_query not in self.pattern_counts:
            return []

        # Sort by frequency
        likely_next = sorted(self.pattern_counts[current_query].items(), key=lambda x: x[1], reverse=True)

        return [query for query, count in likely_next[:top_n]]

    @property
    def hit_rate(self) -> float:
        """Calculate cache hit rate"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0


# ===========================
# Adaptive Batch Processor
# ===========================

class AdaptiveBatchProcessor:
    """Process requests in batches with adaptive sizing"""

    def __init__(self, min_batch_size: int = 5, max_batch_size: int = 20, wait_ms: int = 100):
        self.min_batch_size = min_batch_size
        self.max_batch_size = max_batch_size
        self.current_batch_size = min_batch_size
        self.wait_ms = wait_ms
        self.queue: deque = deque()
        self.processing_times: deque = deque(maxlen=10)

    def adjust_batch_size(self):
        """Dynamically adjust batch size based on queue length and performance"""
        queue_length = len(self.queue)

        # Increase batch size if queue is growing
        if queue_length > 50:
            self.current_batch_size = min(self.max_batch_size, self.current_batch_size + 2)
        # Decrease if queue is small (avoid unnecessary waiting)
        elif queue_length < 10 and self.current_batch_size > self.min_batch_size:
            self.current_batch_size = max(self.min_batch_size, self.current_batch_size - 1)

    async def submit(self, query: str, model: ChatOpenAI) -> str:
        """Submit query for batch processing"""
        future = asyncio.Future()
        self.queue.append((query, model, future))

        self.adjust_batch_size()

        # Process if batch is ready
        if len(self.queue) >= self.current_batch_size:
            await self.process_batch()

        return await future

    async def process_batch(self):
        """Process accumulated queries in parallel"""
        if not self.queue:
            return

        batch_size = min(self.current_batch_size, len(self.queue))
        batch = [self.queue.popleft() for _ in range(batch_size)]

        start_time = time.time()

        # Execute all queries in parallel
        tasks = [self._execute_query(query, model) for query, model, _ in batch]
        responses = await asyncio.gather(*tasks)

        # Resolve futures
        for (query, model, future), response in zip(batch, responses):
            future.set_result(response)

        # Track performance
        batch_time = time.time() - start_time
        self.processing_times.append(batch_time)

    async def _execute_query(self, query: str, model: ChatOpenAI) -> str:
        """Execute single query asynchronously"""
        # Simulate async call (in production, use async LangChain)
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, lambda: model.invoke(query))
        return response.content


# ===========================
# Cost-Aware Router
# ===========================

class CostAwareRouter:
    """Route queries to models with quality fallback"""

    def __init__(self, budget_per_request: float = 0.01, quality_threshold: float = 0.80):
        self.budget = budget_per_request
        self.quality_threshold = quality_threshold
        self.model_costs = {
            "gpt-4": 0.015,  # Estimated average cost per request
            "gpt-4o-mini": 0.003,
            "gpt-3.5-turbo": 0.0005,
        }
        self.fallback_count = 0
        self.total_requests = 0

    def analyze_query_complexity(self, query: str) -> int:
        """Score query complexity 1-10"""
        score = 0

        # Length factor
        word_count = len(query.split())
        score += min(word_count // 10, 3)

        # Complexity keywords
        complex_keywords = [
            "analyze",
            "compare",
            "evaluate",
            "design",
            "architect",
            "critique",
            "argue",
        ]
        medium_keywords = ["explain", "how", "why", "summarize", "describe"]
        simple_keywords = ["what", "define", "who", "when", "where"]

        if any(kw in query.lower() for kw in complex_keywords):
            score += 5
        elif any(kw in query.lower() for kw in medium_keywords):
            score += 3
        elif any(kw in query.lower() for kw in simple_keywords):
            score += 1

        return min(score, 10)

    def select_model(self, query: str) -> tuple[ChatOpenAI, str, float]:
        """Select model based on complexity and budget"""
        complexity = self.analyze_query_complexity(query)
        self.total_requests += 1

        # Route based on complexity
        if complexity >= 8:
            return gpt4, "gpt-4", self.model_costs["gpt-4"]
        elif complexity >= 4:
            return gpt4_mini, "gpt-4o-mini", self.model_costs["gpt-4o-mini"]
        else:
            return gpt35, "gpt-3.5-turbo", self.model_costs["gpt-3.5-turbo"]

    def should_fallback(self, quality_score: float, current_model: str) -> tuple[bool, Optional[str]]:
        """Determine if should fallback to better model"""
        if quality_score < self.quality_threshold:
            # Try next tier up
            if current_model == "gpt-3.5-turbo":
                self.fallback_count += 1
                return True, "gpt-4o-mini"
            elif current_model == "gpt-4o-mini":
                self.fallback_count += 1
                return True, "gpt-4"

        return False, None

    def estimate_quality(self, query: str, response: str) -> float:
        """Estimate response quality (simplified heuristic)"""
        # In production, use LLM-as-judge or human evaluation
        # Here we use simple heuristics

        score = 0.5  # Base score

        # Length check (not too short, not too long)
        response_length = len(response.split())
        if 20 < response_length < 200:
            score += 0.2
        elif response_length < 10:
            score -= 0.2

        # Relevance check (contains keywords from query)
        query_words = set(query.lower().split())
        response_words = set(response.lower().split())
        overlap = len(query_words & response_words)
        score += min(overlap * 0.05, 0.3)

        return min(score, 1.0)


# ===========================
# Advanced Optimized LLM
# ===========================

class AdvancedOptimizedLLM:
    """LLM with advanced optimization strategies"""

    def __init__(self):
        self.cache = PredictiveCache(ttl_seconds=3600)
        self.router = CostAwareRouter(budget_per_request=0.01, quality_threshold=0.75)
        self.total_cost = 0.0
        self.total_latency = 0.0
        self.request_count = 0
        self.model_distribution: dict[str, int] = defaultdict(int)

    def invoke(self, query: str, use_cache: bool = True, use_routing: bool = True) -> dict:
        """Execute LLM call with advanced optimizations"""
        self.request_count += 1
        start_time = time.time()

        # Step 1: Check cache
        if use_cache:
            # Try with different models (check all cached versions)
            for model_name in ["gpt-4", "gpt-4o-mini", "gpt-3.5-turbo"]:
                cached_response = self.cache.get(query, model_name)
                if cached_response:
                    latency = time.time() - start_time
                    self.total_latency += latency
                    self.model_distribution[model_name] += 1

                    return {
                        "response": cached_response,
                        "model": model_name,
                        "latency": latency,
                        "cost": 0.0,
                        "cached": True,
                        "quality_score": 1.0,  # Assume cached responses are good quality
                        "fallback_used": False,
                    }

        # Step 2: Select model based on routing
        if use_routing:
            model, model_name, estimated_cost = self.router.select_model(query)
        else:
            model, model_name, estimated_cost = gpt4, "gpt-4", 0.015

        # Step 3: Execute LLM call
        response = model.invoke(query)
        response_content = response.content

        # Step 4: Evaluate quality
        quality_score = self.router.estimate_quality(query, response_content)

        # Step 5: Fallback to better model if quality insufficient
        fallback_used = False
        if use_routing:
            should_fallback, better_model_name = self.router.should_fallback(quality_score, model_name)

            if should_fallback and better_model_name:
                fallback_used = True
                # Use better model
                if better_model_name == "gpt-4":
                    model = gpt4
                elif better_model_name == "gpt-4o-mini":
                    model = gpt4_mini

                response = model.invoke(query)
                response_content = response.content
                model_name = better_model_name
                estimated_cost = self.router.model_costs[better_model_name]
                quality_score = self.router.estimate_quality(query, response_content)

        # Step 6: Cache response
        if use_cache:
            self.cache.set(query, model_name, response_content)

        # Step 7: Track metrics
        latency = time.time() - start_time
        self.total_latency += latency
        self.total_cost += estimated_cost
        self.model_distribution[model_name] += 1

        return {
            "response": response_content,
            "model": model_name,
            "latency": latency,
            "cost": estimated_cost,
            "cached": False,
            "quality_score": quality_score,
            "fallback_used": fallback_used,
        }

    def prefetch_likely_queries(self, current_query: str):
        """Prefetch likely next queries in background"""
        predicted_queries = self.cache.predict_next_queries(current_query, top_n=2)

        for query in predicted_queries:
            # Check if already cached
            if not self.cache.get(query, "gpt-3.5-turbo"):
                # Prefetch with cheap model
                try:
                    self.invoke(query, use_cache=True, use_routing=True)
                except Exception:
                    pass  # Ignore prefetch failures

    def get_stats(self) -> dict:
        """Get comprehensive statistics"""
        return {
            "total_requests": self.request_count,
            "total_cost": self.total_cost,
            "avg_cost": self.total_cost / self.request_count if self.request_count > 0 else 0,
            "total_latency": self.total_latency,
            "avg_latency": self.total_latency / self.request_count if self.request_count > 0 else 0,
            "cache_hit_rate": self.cache.hit_rate,
            "model_distribution": dict(self.model_distribution),
            "fallback_rate": self.router.fallback_count / self.router.total_requests
            if self.router.total_requests > 0
            else 0,
        }


# ===========================
# A/B Testing Simulation
# ===========================

def simulate_ab_test():
    """Simulate A/B test comparing optimization strategies"""

    print("\n" + "=" * 80)
    print(" " * 25 + "A/B TESTING SIMULATION")
    print("=" * 80)

    # Test queries with varying complexity
    test_queries = [
        "What is Python?",
        "What is Python?",  # Duplicate (test cache)
        "Explain Python programming",  # Similar (test cache)
        "What is 2 + 2?",  # Simple
        "How does machine learning work?",  # Medium
        "Compare supervised and unsupervised learning",  # Complex
        "What is Python?",  # Another duplicate
        "Define artificial intelligence",  # Simple
        "Analyze the trade-offs between SQL and NoSQL databases",  # Complex
        "Explain neural networks",  # Medium
    ]

    # Strategy A: No optimization (baseline)
    print("\nStrategy A: No Optimization (Baseline)")
    print("-" * 80)
    strategy_a = AdvancedOptimizedLLM()
    for query in test_queries:
        result = strategy_a.invoke(query, use_cache=False, use_routing=False)
        print(f"  {query[:50]:<50} | {result['model']:<15} | ${result['cost']:.4f}")

    stats_a = strategy_a.get_stats()

    # Strategy B: Full optimization
    print("\nStrategy B: Full Optimization (Caching + Routing + Fallback)")
    print("-" * 80)
    strategy_b = AdvancedOptimizedLLM()
    for query in test_queries:
        result = strategy_b.invoke(query, use_cache=True, use_routing=True)
        cache_indicator = "âœ…" if result["cached"] else "âŒ"
        fallback_indicator = "â¬†ï¸" if result["fallback_used"] else ""
        print(
            f"  {query[:50]:<50} | {result['model']:<15} | "
            f"${result['cost']:.4f} | Cache: {cache_indicator} {fallback_indicator}"
        )

    stats_b = strategy_b.get_stats()

    # Compare results
    print("\n" + "=" * 80)
    print(" " * 30 + "COMPARISON")
    print("=" * 80)

    print(f"\n{'Metric':<30} {'Strategy A (No Opt)':<25} {'Strategy B (Full Opt)':<25} {'Improvement':<20}")
    print("-" * 100)

    # Cost comparison
    cost_savings = stats_a["total_cost"] - stats_b["total_cost"]
    cost_improvement = (cost_savings / stats_a["total_cost"]) * 100 if stats_a["total_cost"] > 0 else 0
    print(
        f"{'Total Cost':<30} ${stats_a['total_cost']:<24.4f} ${stats_b['total_cost']:<24.4f} "
        f"{cost_improvement:>19.1f}%"
    )

    # Latency comparison
    latency_savings = stats_a["avg_latency"] - stats_b["avg_latency"]
    latency_improvement = (
        (latency_savings / stats_a["avg_latency"]) * 100 if stats_a["avg_latency"] > 0 else 0
    )
    print(
        f"{'Avg Latency':<30} {stats_a['avg_latency']*1000:<24.0f}ms {stats_b['avg_latency']*1000:<24.0f}ms "
        f"{latency_improvement:>19.1f}%"
    )

    # Cache hit rate
    print(
        f"{'Cache Hit Rate':<30} {stats_a['cache_hit_rate']*100:<24.1f}% "
        f"{stats_b['cache_hit_rate']*100:<24.1f}% -"
    )

    # Model distribution
    print(f"\n{'Model Distribution (Strategy B):':<30}")
    for model, count in stats_b["model_distribution"].items():
        percentage = (count / stats_b["total_requests"]) * 100
        print(f"  {model:<28} {count:>3} requests ({percentage:>5.1f}%)")

    # Extrapolate to scale
    print("\n" + "=" * 80)
    print(" " * 25 + "SCALE PROJECTIONS")
    print("=" * 80)

    requests_per_day = 100000
    cost_per_request_a = stats_a["avg_cost"]
    cost_per_request_b = stats_b["avg_cost"]

    daily_cost_a = cost_per_request_a * requests_per_day
    daily_cost_b = cost_per_request_b * requests_per_day
    monthly_savings = (daily_cost_a - daily_cost_b) * 30

    print(f"Daily requests: {requests_per_day:,}")
    print(f"\nStrategy A (No Opt):")
    print(f"  Daily cost: ${daily_cost_a:,.2f}")
    print(f"  Monthly cost: ${daily_cost_a*30:,.2f}")
    print(f"\nStrategy B (Full Opt):")
    print(f"  Daily cost: ${daily_cost_b:,.2f}")
    print(f"  Monthly cost: ${daily_cost_b*30:,.2f}")
    print(f"\nğŸ’° Monthly savings: ${monthly_savings:,.2f} ({cost_improvement:.1f}%)")

    # Winner announcement
    print("\n" + "=" * 80)
    print(" " * 35 + "WINNER")
    print("=" * 80)
    print(f"ğŸ† Strategy B (Full Optimization) wins!")
    print(f"   â€¢ {cost_improvement:.1f}% cost reduction")
    print(f"   â€¢ {latency_improvement:.1f}% latency improvement")
    print(f"   â€¢ {stats_b['cache_hit_rate']*100:.1f}% cache hit rate")
    print(f"   â€¢ {stats_b.get('fallback_rate', 0)*100:.1f}% quality fallback rate")


# ===========================
# Main Demonstration
# ===========================

if __name__ == "__main__":
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘      Resource Optimization - Advanced Implementation          â•‘
    â•‘                                                               â•‘
    â•‘  Demonstrates:                                                â•‘
    â•‘  â€¢ Predictive caching with pattern learning                   â•‘
    â•‘  â€¢ Cost-aware model routing with quality fallback             â•‘
    â•‘  â€¢ Adaptive optimization strategies                           â•‘
    â•‘  â€¢ Request batching (simulated)                               â•‘
    â•‘  â€¢ A/B testing for optimization validation                    â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    simulate_ab_test()

    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                    Key Takeaways                              â•‘
    â•‘                                                               â•‘
    â•‘  âœ… Predictive caching learns query patterns                  â•‘
    â•‘  âœ… Cost-aware routing balances quality and cost              â•‘
    â•‘  âœ… Quality fallback ensures acceptable outputs               â•‘
    â•‘  âœ… Adaptive strategies optimize dynamically                  â•‘
    â•‘  âœ… A/B testing validates optimization impact                 â•‘
    â•‘  âœ… Combined techniques provide 60-80% cost reduction         â•‘
    â•‘                                                               â•‘
    â•‘  Production Ready: Use these patterns at scale!               â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
